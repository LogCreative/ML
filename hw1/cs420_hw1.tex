\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
% \PassOptionsToPackage{numbers, compress}{natbib}
% before loading nips_2017
%
% to avoid loading the natbib package, add option nonatbib:
% \usepackage[nonatbib]{nips_2017}

%\usepackage{nips_2017}
\usepackage[final]{nips_2017.v1} % produce camera-ready copy

% to compile a camera-ready version, add the [final] option, e.g.:
% \usepackage[final]{nips_2017}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{amssymb,amsmath}

\title{Homework 1}

% The \author macro works with any number of authors. There are two
% commands used to separate the names and addresses of multiple
% authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to
% break the lines. Using \AND forces a line break at that point. So,
% if LaTeX puts 3 of 4 authors names on the first line, and the last
% on the second line, try using \AND instead of \And before the third
% author name.

\author{
  CS420 Machine learning 2022 Spring\thanks{tushikui@sjtu.edu.cn} \\
  Department of Computer Science and Engineering\\
  Shanghai Jiao Tong University \\
  %\texttt{hippo@cs.cranberry-lemon.edu} \\
  %% examples of more authors
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
  %% \AND
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
}

\begin{document}
% \nipsfinalcopy is no longer used

\maketitle

%\begin{abstract}
%  \textbf{Submission deadline}: 20:00, April 12, 2018, Thursday \\ \\
%  Please submit your homework in pdf format to the CS420 folder in the following FTP. \\
%  File name should be like this: \texttt{015033910032\_chenyajing\_hw1.pdf}.\\
%  \textbf{ftp://public.sjtu.edu.cn} \\ 
%  \textbf{username:  cyj907}  \\
%  \textbf{password:  public} 
%\end{abstract}

\section*{Submission deadline: 23:59pm, April 10, 2022, Sunday}

\section*{Submission to: }
Please submit your homework in pdf/doc format to Canvas platform. You may also take photos of your hand-written answers and submit the photos to Canvas.
%the CS420 folder in the following FTP. 
%File name should be like this: \texttt{0123456789\_tushikui\_hw1.pdf}.\\
%\begin{center}
% \textbf{ftp://public.sjtu.edu.cn} \\ 
% \textbf{username:  lipeiying}  \\
% \textbf{password:  public} 
%\end{center}


\section{($10$ points) k-mean algorithm}

After initializing the center parameters $\mu_1,\mu_2,\ldots,\mu_K\in\mathbf{R}^n$, the k-mean algorithm is to repeat the following two steps until convergence:
\begin{enumerate}
    \item Assign the points to the nearest $\mu_k$;
    \item Update $\mu_k$ to be the mean of the data points assigned to it.
\end{enumerate}
Prove that each of the above two steps will never increase the k-mean objective function, 
\begin{equation}
    J(\mu_1,\ldots,\mu_K)=\sum_{t=1}^N\sum_{k=1}^K r_{tk}\|x_t-\mu_k\|^2,
\end{equation}
where 
\begin{equation}
    r_{tk} = \left\{ \begin{array}{cc}
        1, & \text{ if } x_t \text{ is assigned to cluster } k; \\ %k={\arg\min}_j \|x_n-\mu_j\|^2; \\
        0, & \text{ otherwise. }
    \end{array} \right.
\end{equation}


\section{($10$ points) k-mean vs GMM}

Give a variant of k-mean algorithm somewhat between the original k-mean and Expectation-Maximization (EM) for Gaussian Mixture Models (GMM). Please specify the computational details of the formulas. Pseudo-codes of the algorithm would be great.

Discuss the advantages or limitations of your algorithm.


\section{($10$ points) k-mean vs CL}

Compare the k-mean algorithm with competitive learning (CL) algorithm. Could you apply the idea of Rival Penalized Competitive Learning (RPCL) to k-mean so that the number of clusters is automatically determined? If so, give the details of your algorithm and then implement it on a three-cluster dataset generated by yourself. If not, state the reasons.


\section{($20$ points) model selection of GMM}

Write a report on experimental comparisons on model selection performance between BIC,AIC and VBEM.

Specifically, you need to randomly generate datasets based on GMM, by varying some factors, e.g., sample sizes, dimensionality, number of clusters, and so on. 
\begin{itemize}

\item BIC, AIC: First, run EM algorithm on each dataset $X$ for $k=1,...,K$, and calculate the log-likelihood value $\ln[p(X|\hat{\Theta}_k)]$, where $\hat{\Theta}_k$ is the maximum likelihood estimate for parameters; Second, select the optimal $k^*$ by
\begin{equation}
    k^*={\arg\max}_{k=1,\ldots,K} {J(k)},
\end{equation}
\begin{equation}
    J_{AIC}(k) = \ln[p(X|\hat{\Theta}_k)] - d_m,
\end{equation}
\begin{equation}
    J_{BIC}(k) = \ln[p(X|\hat{\Theta}_k)] - \frac{\ln N}{2} d_m,
\end{equation}
where $N$ is the number of data points in the dataset, $d_m$ is the number of free parameters in the model, and $K$ is a positive integer specified by the user.

\item Use VBEM algorithm for GMM to select the optimal $k^*$ automatically or via evaluating the lower bound.

\end{itemize}

The following codes might be useful.

Matlab: \url{http://www.cs.ubc.ca/~murphyk/Software/VBEMGMM/index.html}

Python: \url{http://scikit-learn.org/stable/modules/mixture.html}




\end{document}
