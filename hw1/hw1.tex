%&"../ml"
\begin{document}
    \title{第一次作业}
    \maketitle
    
    \section{k-mean 算法}

    \begin{proof}
        对两步分别证明。

        \begin{enumerate}[(a)]
            \item \textbf{E 步} 如果将每一个点 $x_n$ 赋予类 $k_n^\prime$ 使得其相对于其他所有的类最近，即
            \begin{equation*}
                \| x_n-\mu_{k_n^\prime} \| = \min_k{\| x_n - \mu_k\|}
            \end{equation*}
            就意味着它将比上一次赋予的类 $k_n$ 在现在的这种聚类分布下距离不会增加：
            \begin{equation*}
                \| x_n-\mu_{k_n^\prime} \| \leq \| x_n-\mu_{k_n} \|
            \end{equation*}
            那么在对这个点求和的时候，根据指示函数 $r_{nk}$ 的定义，该项也不会增加：
            \begin{equation*}
                j_n = \sum_{k=1}^K r_{nk}^{\prime}\| x_n - \mu_k \|^2 \leq \sum_{k=1}^K r_{nk}\| x_n - \mu_k \|^2
            \end{equation*}
            这里
            \begin{equation*}
                r_{nk} = \begin{cases}
                    1, & \text{$x_n$ 属于类 $k$;}\\
                    0, & \text{其他情况.}
                \end{cases}
            \end{equation*}
            那么损失函数也不会增加：
            \begin{equation}
                J(\mu_1,\cdots,\mu_K) = \sum_{n=1}^N j_n
            \end{equation}
            \item \textbf{M 步} 
            记每个聚类 $k$ 内的点损失函数贡献值为
            \begin{equation}\label{eq:cluster}
                j_k = \sum_{n=1}^N r_{nk}\|x_n-\mu_k\|^2
            \end{equation}
            求和可以交换，损失函数改写为
            \begin{equation}\label{eq:loss}
                J(\mu_1,\cdots,\mu_K)=\sum_{k=1}^K j_k
            \end{equation}
            将用引理 \ref{lem:sqr} 证明，使用聚类内点的平均点作为新的聚类点将不会增加该项。
            \begin{lemma}[距离平方和最小]\label{lem:sqr}
                当 $\mu$ 是所有数据点的均值时，距离平方和
                \begin{equation}\label{eq:dist}
                    f = \sum_{t=1}^N \| \mu-x_t \|^2
                \end{equation}
                最小。
                \begin{proof}
                    将公式 \eqref{eq:dist} 改写
                    \begin{align*}
                        f &= \sum_{t=1}^N \| \mu-x_t \|^2 \\
                          &= \sum_{t=1}^N (\mu-x_t)^T(\mu-x_t) \\
                          &= \sum_{t=1}^N \left(\mu^T\mu - 2x_t^T\mu+x_t^Tx_t\right)
                    \end{align*}
                    对其求导，
                    \begin{align}
                        \frac{\partial f}{\partial \mu} = \sum_{t=1}^N(2\mu^T-2x_t^T)&=0\nonumber\\
                        \mu=\frac{1}{N}\sum_{t=1}^N x_t\label{eq:mean}
                    \end{align}
                    公式 \eqref{eq:mean} 表明当 $\mu$ 是所有数据点的均值时，导数为 0，距离平方和最小。
                \end{proof}
            \end{lemma}
            由于对于每一类而言，公式 \eqref{eq:cluster} 都不会增加，而类别指示函数 $r_{nk}$ 不会改变，所以损失函数 \eqref{eq:loss} 不会增加。
        \end{enumerate}
    \end{proof}

    \section{k-mean 与 GMM 之间}

    \begin{solution}
        为了将 GMM 退化为 k-mean，需要对 GMM 有三个方面的特殊化处理：
        \begin{align}
            \pi_k &= \frac{1}{K} \label{eq:mixing}\\
            \bm{\Sigma} &= \bm{I} \label{eq:conv} \\
            p(k|x_n) &= \begin{cases}
                1, & \text{如果 }k=\arg\max_k\mathcal{N}(x_n|\mu_k,\Sigma_k) \\
                0, & \text{其他情况.}
            \end{cases} \label{eq:oneink}
        \end{align}
        公式 \eqref{eq:mixing} 是混合权重的归一，公式 \eqref{eq:conv} 是协方差一致使得其只计算欧氏距离，公式 \eqref{eq:oneink} 是硬赋值只取可能性最大的那个聚类 $k$。

        为了得到其中一个中间变种，这里我们只退化 \eqref{eq:mixing}，使 GMM 中的 $\pi_k=\frac{1}{K}$，就可以得到一个更加一般的带方差项软赋值的 k-mean 算法 \ref{alg:vark}。此时的对数似然值定义为
        \begin{equation}\label{eq:loglike}
            \ln p(\bm{X}|\bm{\mu},\bm{\Sigma})=\sum_{n=1}^N\left[-\ln K + \ln\sum_{k=1}^K\mathcal{N}(\bm{x}_n|\bm{\mu}_k,\bm{\Sigma}_k)\right]
        \end{equation}

        \textbf{优点} 这种算法可以很好地拓展 k-mean 算法，使其能够具有方差项（引入高斯分布），并且软赋值可以更好地考虑多个聚类。
        
        \textbf{缺点} 这种算法无疑增加了一定的计算量。
    \end{solution}

    \begin{algorithm}
        \caption{含方差软赋值的 k-mean 算法}\label{alg:vark}
        \KwIn{数据点 $\bm{X}={\bm{x}_n}_{n=1}^N$, 聚类数目 $K$}
        \KwOut{聚类的分类结果 $\bm{\mu}_j,\bm{\Sigma}_j,\quad\forall j\in \mathbb{N}\cap[1,K]$}
        \BlankLine
        初始化均值矩阵 $\bm{\mu}_k$，协方差矩阵 $\bm{\Sigma}_k$，根据公式 \eqref{eq:loglike} 初始化对数似然值\;
        \Repeat{公式 \eqref{eq:loglike} 中的值没有明显变化}{
            \For{$n\leftarrow 1$ to $N$}{
                \For{$k\leftarrow 1$ to $K$}{
                    $\gamma_{nk}^{(t)}\leftarrow \frac{\mathcal{N}(\bm{x}_n|\bm{\mu}_k,\bm{\Sigma}_k)}{\sum_{j=1}^K\mathcal{N}(\bm{x}_n|\bm{\mu}_j,\bm{\Sigma}_j)}$\tcc*[r]{E 步}
                }
            }
            \For{$k\leftarrow 1$ to $K$}{
                $\bm{\mu}_k^{(t+1)}\leftarrow\frac{\sum_{n=1}^N\gamma_{nk}^{(t)}\bm{x}_n}{\sum_{n=1}^N\gamma_{nk}}$\;
                $\bm{\Sigma}_k^{(t+1)}\leftarrow\frac{\sum_{n=1}^N\gamma_{nk}^{(t)}(\bm{x}_n-\bm{\mu}_k^{(t+1)})(\bm{x}_n-\bm{\mu}_k^{(t+1)})^T}{\sum_{n=1}^N\gamma_{nk}}$\tcc*[r]{M 步}
            }
        }
    \end{algorithm}

    \section{k-mean 与 CL}
\end{document}